{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self, col, split, lchild, rchild):\n",
    "        self.col = col\n",
    "        self.split = split,\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        if X_test[self.col] <= self.split:\n",
    "            return self.lchild.predict(X_test)\n",
    "        return self.rchild.predict(X_test)\n",
    "    \n",
    "    def leaf(self, X_test):\n",
    "        \"\"\"\n",
    "        Given a single test record, x_test, return the leaf node reached by running\n",
    "        it down the tree starting at this node.  This is just like prediction,\n",
    "        except we return the decision tree leaf rather than the prediction from that leaf.\n",
    "        \"\"\"\n",
    "        return self.predict(X_test) # trigger leafnode prediction (return leaf)\n",
    "\n",
    "\n",
    "class LeafNode:\n",
    "    def __init__(self, y, prediction):\n",
    "        self.y = y\n",
    "        self.n = len(y)\n",
    "        self.prediction = prediction\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # when reaching the leaf return the leaf it self\n",
    "        # include x_test just wanna keep the interface constant as DecisionNode.predict\n",
    "        if self.prediction: return self\n",
    "            \n",
    "\n",
    "class RandomForest621:\n",
    "    def __init__(self, n_estimators=10, oob_score=False):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.oob_score = oob_score\n",
    "        self.oob_score_ = np.nan\n",
    "        \n",
    "    def _RFBestSplit(self, X, y, loss, max_features):\n",
    "        \"\"\"\n",
    "        Given an (X, y) training set, fit all n_estimators trees to different,\n",
    "        bootstrapped versions of the training data.  Keep track of the indexes of\n",
    "        the OOB records for each tree.  After fitting all of the trees in the forest,\n",
    "        compute the OOB validation score estimate and store as self.oob_score_, to\n",
    "        mimic sklearn.\n",
    "        \"\"\"\n",
    "        best = -1, -1, loss(y) # col, split, loss\n",
    "        feature_subset_idx = np.random.choice(X.shape[1], # selecting column feature index subset\n",
    "                                                # do not use floor, not able to pass the unit test r^2 low for some re\n",
    "                                                math.ceil(max_features * X.shape[1]), replace=True)\n",
    "        for col in feature_subset_idx:\n",
    "            X_col = X[:, col]\n",
    "            # randomly choose 11 point as split spots and find the bast one with lowest gini or std\n",
    "            split_candidate  = np.random.choice(X_col, 15)\n",
    "            for split in split_candidate:\n",
    "                yl, yr = y[X_col <= split], y[X_col > split] #<= is important\n",
    "                yl_size, yr_size = len(yl), len(yr)\n",
    "                if yl_size <= self.min_samples_leaf or yr_size <= self.min_samples_leaf:\n",
    "                    continue\n",
    "                l = (yl_size * loss(yl) + yr_size * loss(yr)) / (yl_size + yr_size)\n",
    "                if l == 0: return best[0], best[1] # return col, and split_candidate\n",
    "                if l < best[-1]: best = col, split, l\n",
    "        return best[0], best[1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Given an (X, y) training set, fit all n_estimators trees to different,\n",
    "        bootstrapped versions of the training data.  Keep track of the indexes of\n",
    "        the OOB records for each tree.  After fitting all of the trees in the forest,\n",
    "        compute the OOB validation score estimate and store as self.oob_score_, to\n",
    "        mimic sklearn.\n",
    "        \"\"\"\n",
    "        self.trees_ = []\n",
    "        for i in range(self.n_estimators):\n",
    "            inbagX, inbagy, oobX, ooby = bootStrapping(X=X, y=y)\n",
    "            tree = self._fit(X=inbagX, y=inbagy)\n",
    "            self.trees_.append(tree)\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        self.nclass = np.unique(y)\n",
    "        if len(X) <= self.min_samples_leaf or len(self.nclass)==1:\n",
    "            return LeafNode(y=y, prediction=self.prediction)\n",
    "        col, split = self._RFBestSplit(X, y, loss=self.loss, max_features=self.max_features)\n",
    "        if col == -1: return LeafNode(y=y, prediction=self.prediction)\n",
    "        lchild = self._fit(X = X[X[:, col]<=split], y=y[X[:, col]<=split])\n",
    "        rchild = self._fit(X = X[X[:, col]>split], y=y[X[:, col]>split])\n",
    "        return DecisionNode(col=col, split=split,lchild = lchild, rchild =rchild)\n",
    "    \n",
    "\n",
    "class RandomForestRegressor621(RandomForest621):\n",
    "    def __init__(self, n_estimators=10, min_samples_leaf=3, max_features=0.3, oob_score=False):\n",
    "        super().__init__(n_estimators, oob_score=oob_score)\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.prediction = \"regress\"\n",
    "        self.oob_score = oob_score\n",
    "        self.loss = np.std\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Given a 2D nxp array with one or more records, compute the weighted average\n",
    "        prediction from all trees in this forest. Weight each trees prediction by\n",
    "        the number of samples in the leaf making that prediction.  Return a 1D vector\n",
    "        with the predictions for each input record of X_test. return numpy array.\n",
    "        \"\"\"    \n",
    "        y_pred = np.zeros(len(X_test))\n",
    "        for row_idx, row in enumerate(X_test):  #for each row in X-test\n",
    "            leaves_size, ypred_sum = 0, 0\n",
    "            for tree in self.trees_:\n",
    "                myleaf = tree.leaf(row)\n",
    "                ypred_sum += np.mean(myleaf.y) * myleaf.n\n",
    "                leaves_size += myleaf.n\n",
    "            y_pred[row_idx] = ypred_sum / leaves_size\n",
    "        return y_pred\n",
    "        \n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Given a 2D nxp X_test array and 1D nx1 y_test array with one or more records,\n",
    "        collect the prediction for each record and then compute R^2 on that and y_test.\n",
    "        \"\"\"\n",
    "        return r2_score(y_true=y_test, y_pred=self.predict(X_test))\n",
    "\n",
    "\n",
    "class RandomForestClassifier621(RandomForest621):\n",
    "    def __init__(self, n_estimators=10, min_samples_leaf=3, max_features=0.3, oob_score=False):\n",
    "        super().__init__(n_estimators, oob_score=oob_score)\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.prediction = \"classify\"\n",
    "        self.oob_score = oob_score\n",
    "        self.loss = gini\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = np.zeros(len(X_test))\n",
    "        for counter_idx in range(len(X_test)):\n",
    "            count_dict = {}\n",
    "            for tree in self.trees_:\n",
    "                myleaf = tree.leaf(X_test[counter_idx, :]) # get leaf for single x_test \n",
    "                class_indices, count = np.unique(myleaf.y, return_counts=True)\n",
    "                for class_idx, freq in zip(class_indices, count):\n",
    "                    count_dict[class_idx] = count_dict.get(class_idx, 0) + freq\n",
    "                # getting the class with the highest freq\n",
    "                y_pred[counter_idx] = max(count_dict, key=count_dict.get)     \n",
    "        return y_pred\n",
    "        \n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Given a 2D nxp X_test array and 1D nx1 y_test array with one or more records,\n",
    "        collect the predicted class for each record and then compute accuracy between\n",
    "        that and y_test.\n",
    "        \"\"\"\n",
    "        return r2_score(y_true=y_test, y_pred=self.predict(X_test))\n",
    "\n",
    "def bootStrapping(X, y):\n",
    "    \"\"\"bootstrapping sample with replacement\n",
    "    And Return the bootstrapped sample size = orginal data size\n",
    "\n",
    "    Args:\n",
    "        X (2D numpy array): [n*p matrix]\n",
    "        y (1D numpy array): [array]\n",
    "    \"\"\" \n",
    "    n = len(X)\n",
    "    mask = np.ones(n, dtype=bool)\n",
    "    inbag_idx = np.random.randint(0, n, size=n)\n",
    "    mask[inbag_idx] = False\n",
    "    inBagX, inBagy = X[inbag_idx], y[inbag_idx] # in-bag-sample\n",
    "    outofBagX, outofBagy = X[mask], y[mask] # out-of-bag sample\n",
    "    return inBagX, inBagy, outofBagX, outofBagy\n",
    "\n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Compute gini impurity from y vector of class values (from k unique values).\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / len(y) # p: probablity ditribution (numpy array)\n",
    "    return 1 - np.sum( p**2 )\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}