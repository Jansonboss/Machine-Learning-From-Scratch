{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, col, split, lchild, rchild):\n",
    "        self.col = col\n",
    "        self.split = split\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "    def predict(self , X_test):\n",
    "        # Make decicion based upon x_test[col] and split\n",
    "        # predict on root node, and call leafNode.predict\n",
    "        if X_test[self.col] <= self.split:\n",
    "            return self.lchild.predict(X_test)\n",
    "        return self.rchild.predict(X_test)\n",
    "\n",
    "class LeafNode:\n",
    "    def __init__(self, y, prediction):\n",
    "        \"\"\"\n",
    "        Since we have prediction, we know it is leafnode; Create leaf node from y values\n",
    "        prediction is mean(y) or mode(y)\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.n = len(y)\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        # return prediction\n",
    "        if self.prediction == \"regress\":\n",
    "            return np.mean(self.y)\n",
    "        return mode(self.y)[0][0]\n",
    "\n",
    "\n",
    "class DecisionTree621:\n",
    "    def __init__(self, min_samples_leaf=1, loss=None):\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.loss = loss # loss function; either np.std or gini\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Create a decision tree fit to (X,y) and save as self.root, the root of\n",
    "        our decision tree, for either a classifier or regressor.  Leaf nodes for classifiers\n",
    "        predict the most common class (the mode) and regressors predict the average y\n",
    "        for samples in that leaf.\n",
    "\n",
    "        This function is a wrapper around fit_() that just stores the tree in self.root.\n",
    "        \"\"\"\n",
    "        self.root = self.fit_(X, y)\n",
    "\n",
    "    def fit_(self, X, y):\n",
    "        \"\"\"\n",
    "        Recursively create and return a decision tree fit to (X,y) for\n",
    "        either a classifier or regressor.  This function should call self.create_leaf(X,y)\n",
    "        to create the appropriate leaf node, which will invoke either\n",
    "        RegressionTree621.create_leaf() or ClassifierTree621. create_leaf() depending\n",
    "        on the type of self.\n",
    "\n",
    "        This function is not part of the class \"interface\" and is for internal use, but it\n",
    "        embodies the decision tree fitting algorithm.\n",
    "\n",
    "        (Make sure to call fit_() not fit() recursively.)\n",
    "        \"\"\"\n",
    "\n",
    "        # Base case: reach the smallest sized node => create a leaf node\n",
    "        if len(X) <= self.min_samples_leaf or len(np.unique(X))==1:\n",
    "            return self.create_leaf(y=y) # return predction leaf\n",
    "\n",
    "        col, split = self._bestsplit(X=X, y=y, loss=self.loss) # column_index, and split point\n",
    "        if col == -1: # no best split found, return the whole y as leadNode(prediction)\n",
    "            return self.create_leaf(y=y)\n",
    "\n",
    "        # create left subtree and right subtree recursively\n",
    "        lchild, rchild = self.fit_(X=X[X[:,col]<=split], y=y[X[:,col]<=split]), self.fit_(X=X[X[:,col]>split], y=y[X[:,col]>split])\n",
    "        return DecisionNode(col, split, lchild, rchild) # decision node\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Make a prediction for each record in X_test and return as array.\n",
    "        This method is inherited by RegressionTree621 and ClassifierTree621 and\n",
    "        works for both without modification! -> call predict on the root Node\n",
    "        \"\"\"\n",
    "        preds = np.empty(X_test.shape[0])\n",
    "        for i in range(X_test.shape[0]):\n",
    "            preds[i] = self.root.predict(X_test[i])\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def _bestsplit(self, X, y, loss):\n",
    "        \"\"\"\n",
    "        find the best spot to split the data so that global loss is minimized\n",
    "        for each features col: col_x in X, randomly select 11 unique points from col_x\n",
    "        as split point (skip the split sample num in some leaves is too small), find the\n",
    "        best split\n",
    "\n",
    "        return col seletced and split\n",
    "        \"\"\"\n",
    "        best = (-1, -1, loss(y)) ## initialize the best comb\n",
    "        for col in range(X.shape[1]): # loop through col index of X\n",
    "            col_x = X[:, col] # get column data\n",
    "            candidate = np.random.choice(col_x, 11)\n",
    "            for split in candidate:\n",
    "                ly, ry = y[col_x <= split], y[col_x > split]\n",
    "                ly_size, ry_size, y_size = len(ly), len(ry), len(y)\n",
    "\n",
    "                # find feature space larger than predefined minimum sample cardinality\n",
    "                if ly_size < self.min_samples_leaf or ry_size < self.min_samples_leaf:\n",
    "                    continue # jump outof the for loop, avoid overfiiting\n",
    "\n",
    "                l = (ly_size * loss(ly) + ry_size * loss(ry)) / y_size # weight average of loss\n",
    "                if l ==0: return col, split\n",
    "                if l < best[2]: best = (col, split, l) # update the res\n",
    "        return best[0], best[1]\n",
    "\n",
    "\n",
    "class RegressionTree621(DecisionTree621):\n",
    "    def __init__(self, min_samples_leaf=1):\n",
    "        super().__init__(min_samples_leaf, loss=np.std)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        \"Return the R^2 of y_test vs predictions for each record in X_test\"\n",
    "        return r2_score(y_true=y_test, y_pred=self.predict(X_test=X_test))\n",
    "\n",
    "    def create_leaf(self, y):\n",
    "        \"\"\"\n",
    "        Return a new LeafNode for regression, passing y and mean(y) to\n",
    "        the LeafNode constructor.\n",
    "        \"\"\"\n",
    "        return LeafNode(y=y, prediction=\"regress\")\n",
    "\n",
    "\n",
    "class ClassifierTree621(DecisionTree621):\n",
    "    def __init__(self, min_samples_leaf=1):\n",
    "        super().__init__(min_samples_leaf, loss=gini)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_true=y_test, y_pred=self.predict(X_test=X_test))\n",
    "\n",
    "    def create_leaf(self, y):\n",
    "        \"\"\"\n",
    "        Return a new LeafNode for classification, passing y and mode(y) to\n",
    "        the LeafNode constructor.\n",
    "        \"\"\"\n",
    "        return LeafNode(y=y, prediction='classify')\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Compute gini impurity from y vector of class values (from k unique values).\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / len(y) # p: probablity ditribution (numpy array)\n",
    "    return 1 - np.sum( p**2 )\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}